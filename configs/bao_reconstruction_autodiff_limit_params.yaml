# MadEvolve Configuration: BAO Reconstruction with Autodiff (Limit Params)
#
# Run with:
#   madevolve run -c configs/bao_reconstruction_autodiff_limit_params.yaml -o ./results/bao_autodiff

# Task definition
task_description: |
  You are a scientific pioneer and expert numerical cosmologist at the bleeding edge of your field.
  Your mission is to **invent radically new 3-D BAO reconstruction algorithms that challenge and
  surpass the current state-of-the-art**, including the canonical Zel'dovich "standard reconstruction."

  **Core Objective (Absolute Priority):**
  * **Maximize the cross-correlation coefficient r(k)** between the reconstructed and ground truth
    density fields. This is the ultimate measure of BAO reconstruction success, as it directly
    quantifies how well the algorithm recovers the initial density field at BAO scales (k ~ 0.01-0.5 h/Mpc).

  **CRITICAL: Preserve Large-Scale Structure (Small k) - PER-BIN CONSTRAINT**
  * For small k values (k ~ 0.01-0.2 h/Mpc, corresponding to large spatial scales), r(k) should
    remain **very high, close to 1.0** and must NOT degrade from the baseline algorithm.
  * The baseline r(k) values at each k bin are:
    - k=0.022: 0.999, k=0.047: 0.998, k=0.071: 0.997, k=0.096: 0.996
    - k=0.120: 0.995, k=0.145: 0.988, k=0.169: 0.972, k=0.194: 0.946
  * **PENALTY: If ANY k bin in [0.01, 0.2] performs worse than baseline, penalty = 10 x max_degradation**
    - Example: If your r(k) at k=0.047 is 0.95 (baseline=0.998), degradation=0.048, penalty=0.48
    - This penalty is subtracted from your score, making it very costly to sacrifice ANY k bin.
  * Focus on improving small-scale (large k > 0.2) reconstruction WITHOUT hurting ANY large-scale bin.

  **CRITICAL REQUIREMENT: FULLY DIFFERENTIABLE CODE (JAX)**
  This evolution run uses **Autodiff Parameter Optimization**. Your code MUST be fully differentiable
  using JAX operations. This means:

  1. **Use jax.numpy (jnp) instead of numpy (np)** for ALL numerical operations in the EVOLVE-BLOCK
  2. **Avoid non-differentiable operations:**
     - No `if/else` based on array values (use `jnp.where` instead)
     - No `for` loops over array elements (use vectorized operations)
     - No `.astype(int)` for indexing (this breaks gradients) - use soft indexing or interpolation
     - No in-place modifications (JAX arrays are immutable)

  3. **TUNABLE Parameters with Autodiff:**
     ```python
     # TUNABLE: param_name = default_value, bounds=(min, max), method=autodiff

     def my_function(data, param_name=default_value):
         # Use param_name in differentiable operations
         result = jnp.exp(-data * param_name)  # Gradients can flow through this
         return result
     ```

  4. **Examples of differentiable vs non-differentiable code:**
     ```python
     # BAD (non-differentiable):
     idx = (pos / cell_size).astype(int)  # .astype(int) breaks gradients
     grid[idx] = value  # In-place assignment breaks gradients

     # GOOD (differentiable):
     weights = jnp.exp(-distance**2 / sigma**2)  # Smooth, differentiable
     result = jnp.sum(values * weights)  # Differentiable reduction
     ```

  **Why Autodiff?**
  - Gradient-based optimization finds optimal parameters MUCH faster than grid search
  - Your algorithm's parameters will be automatically tuned using gradient descent
  - This allows exploring algorithms with many tunable parameters efficiently

  **HARD CONSTRAINT: MAXIMUM 10 TUNABLE PARAMETERS**
  Your program MUST NOT have more than 10 TUNABLE parameters. This is a strict requirement.
  - Programs with more than 10 parameters will be REJECTED
  - Focus on the most impactful parameters rather than adding many small ones
  - Quality over quantity: choose parameters that meaningfully affect the reconstruction

  **Embrace Radical Innovation: Think Beyond the Literature**
  Your main task is to generate novel ideas that are **not found in existing cosmology literature**.
  Be bold, creative, and unconventional. Consider:

  *   **Differentiable Physics:** Implement physics-based constraints that are differentiable
  *   **Soft Assignments:** Replace hard binning/indexing with soft, differentiable alternatives
  *   **Neural-Inspired Architectures:** Use differentiable operations inspired by neural networks
  *   **Multi-scale Approaches:** Different smoothing scales for different k-modes, all differentiable
  *   **Iterative Refinement:** Differentiable iterative algorithms with learnable step sizes

  **Problem Context & Constants:**
  3-D periodic simulation boxes:
  ```python
  BoxSize = 1000    # Mpc/h
  NMesh = 256       # grid resolution
  Kf = 2 * jnp.pi / BoxSize
  ```

  **IMPORTANT: Isotropy**
  The underlying cosmological density field is **statistically isotropic** - there is no preferred direction.
  Your reconstruction algorithm should respect this symmetry:
  - Do NOT treat x, y, z directions differently
  - Any smoothing, filtering, or weighting should be the same in all directions
  - Use isotropic kernels (e.g., spherically symmetric Gaussians)
  - Parameters should not depend on spatial direction

  **Input/Output:**
  - Input: 3D density field (256^3 array)
  - Output: Reconstructed initial density field (256^3 array)
  - Evaluation: Cross-correlation r(k) in BAO range (k ~ 0.05-0.5 h/Mpc)

  **Allowed Tools:**
  * You MUST use JAX (`jax`, `jax.numpy as jnp`) for all differentiable operations
  * You may use `scipy` for non-differentiable utilities (file I/O, etc.)
  * Always include explicit `import` statements for self-contained code

  **Key Performance Metric:**
  Average r(k) in BAO range [0.01, 0.5] h/Mpc is the primary score.
  Higher values (closer to 1.0) indicate better reconstruction quality.

  **Scoring with Per-Bin Large-Scale Penalty:**
  - Base score = average r(k) over [0.01, 0.5] h/Mpc
  - For each k bin in [0.01, 0.2], compute degradation = max(0, baseline_r(k) - your_r(k))
  - Penalty = 10 x max(degradation across all 8 bins)
  - Combined score = base_score - penalty
  - This ensures you cannot sacrifice ANY single k bin for gains elsewhere.

evaluator_script: "examples/bao_reconstruction_auto_diff/evaluate_autodiff.py"
init_program_path: "examples/bao_reconstruction_auto_diff/initial_autodiff.py"

# Evolution parameters
num_generations: 10000

# LLM model configuration
models:
  models:
    - "gemini-3-pro-preview"
    - "gpt-5.2"
    - "gemini-3-flash-preview"
    - "o4-mini"
  weights:
    - 0.25
    - 0.25
    - 0.25
    - 0.25
  temperature: 0.7
  max_tokens: 16384
  adaptive_selection: true
  selection_algorithm: "ucb"
  exploration_factor: 1.0

# Patch policy (diff, holistic, synthesis = cross)
patch_policy:
  modes:
    - "differential"
    - "holistic"
    - "synthesis"
  weights:
    - 0.4
    - 0.4
    - 0.2
  adaptive: true
  stagnation_threshold: 10

# Population configuration
population:
  islands:
    enabled: true
    num_islands: 4
    island_capacity: 15
    migration_interval: 5
    migration_rate: 0.1
  archive:
    enabled: true
    max_size: 50
    elite_count: 10
  selection_strategy: "adaptive"
  exploitation_ratio: 0.6

# Inner-loop parameter optimization
# Note: The actual autodiff optimization is handled by evaluate_autodiff.py
# which uses JAX gradients. This MadEvolve optimization is for the outer loop.
optimization:
  enabled: true
  max_budget: 20

# Executor configuration
executor:
  mode: "local"
  max_parallel_jobs: 4
  timeout: 600.0

# Storage
storage:
  db_path: "evolution.db"
  checkpoint_interval: 5
  keep_all_programs: true

# Reporting
report:
  enabled: true
  include_lineage: true
  include_analysis: true

# Prompt settings
include_text_feedback: true
include_metrics_history: true
